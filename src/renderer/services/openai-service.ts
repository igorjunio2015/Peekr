import OpenAI from 'openai'
import { AIMessage } from '@shared/types'

export interface StreamOptions {
  model?: string
  temperature?: number
  maxTokens?: number
  systemPrompt?: string
}

// Interface para informa√ß√µes de uso de tokens
export interface TokenUsage {
  promptTokens: number      // Tokens enviados (input)
  completionTokens: number  // Tokens recebidos (output)
  totalTokens: number       // Total
  cachedTokens?: number     // Tokens em cache (se aplic√°vel)
  cost?: CostInfo           // Custo calculado
}

// Interface para informa√ß√µes de custo
export interface CostInfo {
  inputCost: number         // Custo dos tokens de entrada em USD
  outputCost: number        // Custo dos tokens de sa√≠da em USD
  totalCost: number         // Custo total em USD
  model: string             // Modelo usado
}

// Pre√ßos por 1M tokens (em USD) - Atualizado Jan 2025
// Fonte: https://openai.com/pricing
export const MODEL_PRICING: Record<string, { input: number; output: number; cached?: number }> = {
  // GPT-4o Series
  'gpt-4o': { input: 2.50, output: 10.00, cached: 1.25 },
  'gpt-4o-2024-08-06': { input: 2.50, output: 10.00, cached: 1.25 },
  'gpt-4o-2024-05-13': { input: 5.00, output: 15.00 },
  'gpt-4o-mini': { input: 0.15, output: 0.60, cached: 0.075 },
  'gpt-4o-mini-2024-07-18': { input: 0.15, output: 0.60, cached: 0.075 },
  'chatgpt-4o-latest': { input: 5.00, output: 15.00 },
  
  // GPT-4 Turbo Series
  'gpt-4-turbo': { input: 10.00, output: 30.00 },
  'gpt-4-turbo-2024-04-09': { input: 10.00, output: 30.00 },
  'gpt-4-turbo-preview': { input: 10.00, output: 30.00 },
  
  // GPT-4 Series
  'gpt-4': { input: 30.00, output: 60.00 },
  'gpt-4-0613': { input: 30.00, output: 60.00 },
  'gpt-4-32k': { input: 60.00, output: 120.00 },
  'gpt-4-32k-0613': { input: 60.00, output: 120.00 },
  
  // GPT-3.5 Series
  'gpt-3.5-turbo': { input: 0.50, output: 1.50 },
  'gpt-3.5-turbo-0125': { input: 0.50, output: 1.50 },
  'gpt-3.5-turbo-1106': { input: 1.00, output: 2.00 },
  'gpt-3.5-turbo-16k': { input: 3.00, output: 4.00 },
  
  // o1 Series (Reasoning Models)
  'o1': { input: 15.00, output: 60.00, cached: 7.50 },
  'o1-2024-12-17': { input: 15.00, output: 60.00, cached: 7.50 },
  'o1-preview': { input: 15.00, output: 60.00 },
  'o1-preview-2024-09-12': { input: 15.00, output: 60.00 },
  'o1-mini': { input: 3.00, output: 12.00, cached: 1.50 },
  'o1-mini-2024-09-12': { input: 3.00, output: 12.00, cached: 1.50 },
  
  // o3 Series
  'o3-mini': { input: 1.10, output: 4.40, cached: 0.55 },
  'o3-mini-2025-01-31': { input: 1.10, output: 4.40, cached: 0.55 },
  
  // Whisper (Audio) - pre√ßo por minuto convertido para tokens aproximados
  // $0.006/min ‚âà $0.0001/segundo ‚âà ~150 tokens/segundo
  'whisper-1': { input: 0.006, output: 0 }, // Pre√ßo especial por minuto
  
  // Vision (imagens) - custo adicional por imagem
  // Low detail: $0.00255, High detail: $0.00765 base + $0.00255 por tile
  'vision-low': { input: 2.55, output: 0 },
  'vision-high': { input: 7.65, output: 0 },
}

/**
 * Calcula o custo de uma requisi√ß√£o baseado no uso de tokens
 */
export function calculateCost(
  model: string,
  promptTokens: number,
  completionTokens: number,
  cachedTokens: number = 0
): CostInfo {
  // Obter pre√ßos do modelo (fallback para gpt-4o se n√£o encontrar)
  const pricing = MODEL_PRICING[model] || MODEL_PRICING['gpt-4o']
  
  // Calcular tokens n√£o-cached
  const nonCachedPromptTokens = promptTokens - cachedTokens
  
  // Calcular custos (pre√ßos s√£o por 1M tokens)
  const cachedCost = cachedTokens > 0 && pricing.cached
    ? (cachedTokens / 1_000_000) * pricing.cached
    : 0
  const inputCost = (nonCachedPromptTokens / 1_000_000) * pricing.input + cachedCost
  const outputCost = (completionTokens / 1_000_000) * pricing.output
  const totalCost = inputCost + outputCost
  
  return {
    inputCost,
    outputCost,
    totalCost,
    model
  }
}

/**
 * Calcula o custo de transcri√ß√£o de √°udio (Whisper)
 * @param durationSeconds Dura√ß√£o do √°udio em segundos
 */
export function calculateWhisperCost(durationSeconds: number): CostInfo {
  // Whisper cobra $0.006 por minuto
  const minutes = durationSeconds / 60
  const totalCost = minutes * 0.006
  
  return {
    inputCost: totalCost,
    outputCost: 0,
    totalCost,
    model: 'whisper-1'
  }
}

/**
 * Calcula o custo de an√°lise de imagem (Vision)
 * @param detail 'low' ou 'high' - n√≠vel de detalhe da an√°lise
 * @param tiles N√∫mero de tiles para high detail (1 tile = 512x512)
 */
export function calculateVisionCost(detail: 'low' | 'high' = 'high', tiles: number = 1): CostInfo {
  let totalCost: number
  
  if (detail === 'low') {
    totalCost = 0.00255 // Custo fixo para low detail
  } else {
    // High detail: base + custo por tile
    totalCost = 0.00765 + (tiles * 0.00255)
  }
  
  return {
    inputCost: totalCost,
    outputCost: 0,
    totalCost,
    model: `vision-${detail}`
  }
}

// Informa√ß√µes de contexto dos modelos OpenAI (n√£o dispon√≠vel via API)
// Fonte: https://platform.openai.com/docs/models
export interface ModelContextInfo {
  contextWindow: number  // Total de tokens dispon√≠veis
  maxOutput: number      // M√°ximo de tokens de sa√≠da
  name: string           // Nome amig√°vel
  supportsVision?: boolean
  supportsAudio?: boolean
}

export const MODEL_CONTEXT_INFO: Record<string, ModelContextInfo> = {
  // GPT-4o Series
  'gpt-4o': {
    contextWindow: 128000,
    maxOutput: 16384,
    name: 'GPT-4o',
    supportsVision: true
  },
  'gpt-4o-2024-08-06': {
    contextWindow: 128000,
    maxOutput: 16384,
    name: 'GPT-4o (Aug 2024)',
    supportsVision: true
  },
  'gpt-4o-2024-05-13': {
    contextWindow: 128000,
    maxOutput: 4096,
    name: 'GPT-4o (May 2024)',
    supportsVision: true
  },
  'gpt-4o-mini': {
    contextWindow: 128000,
    maxOutput: 16384,
    name: 'GPT-4o Mini',
    supportsVision: true
  },
  'gpt-4o-mini-2024-07-18': {
    contextWindow: 128000,
    maxOutput: 16384,
    name: 'GPT-4o Mini (Jul 2024)',
    supportsVision: true
  },
  'chatgpt-4o-latest': {
    contextWindow: 128000,
    maxOutput: 16384,
    name: 'ChatGPT-4o Latest',
    supportsVision: true
  },
  
  // GPT-4 Turbo Series
  'gpt-4-turbo': {
    contextWindow: 128000,
    maxOutput: 4096,
    name: 'GPT-4 Turbo',
    supportsVision: true
  },
  'gpt-4-turbo-2024-04-09': {
    contextWindow: 128000,
    maxOutput: 4096,
    name: 'GPT-4 Turbo (Apr 2024)',
    supportsVision: true
  },
  'gpt-4-turbo-preview': {
    contextWindow: 128000,
    maxOutput: 4096,
    name: 'GPT-4 Turbo Preview',
    supportsVision: false
  },
  
  // GPT-4 Series
  'gpt-4': {
    contextWindow: 8192,
    maxOutput: 4096,
    name: 'GPT-4'
  },
  'gpt-4-0613': {
    contextWindow: 8192,
    maxOutput: 4096,
    name: 'GPT-4 (Jun 2023)'
  },
  'gpt-4-32k': {
    contextWindow: 32768,
    maxOutput: 4096,
    name: 'GPT-4 32K'
  },
  'gpt-4-32k-0613': {
    contextWindow: 32768,
    maxOutput: 4096,
    name: 'GPT-4 32K (Jun 2023)'
  },
  
  // GPT-3.5 Series
  'gpt-3.5-turbo': {
    contextWindow: 16385,
    maxOutput: 4096,
    name: 'GPT-3.5 Turbo'
  },
  'gpt-3.5-turbo-0125': {
    contextWindow: 16385,
    maxOutput: 4096,
    name: 'GPT-3.5 Turbo (Jan 2025)'
  },
  'gpt-3.5-turbo-1106': {
    contextWindow: 16385,
    maxOutput: 4096,
    name: 'GPT-3.5 Turbo (Nov 2023)'
  },
  'gpt-3.5-turbo-16k': {
    contextWindow: 16385,
    maxOutput: 4096,
    name: 'GPT-3.5 Turbo 16K'
  },
  
  // o1 Series (Reasoning Models)
  'o1': {
    contextWindow: 200000,
    maxOutput: 100000,
    name: 'o1'
  },
  'o1-2024-12-17': {
    contextWindow: 200000,
    maxOutput: 100000,
    name: 'o1 (Dec 2024)'
  },
  'o1-preview': {
    contextWindow: 128000,
    maxOutput: 32768,
    name: 'o1 Preview'
  },
  'o1-preview-2024-09-12': {
    contextWindow: 128000,
    maxOutput: 32768,
    name: 'o1 Preview (Sep 2024)'
  },
  'o1-mini': {
    contextWindow: 128000,
    maxOutput: 65536,
    name: 'o1 Mini'
  },
  'o1-mini-2024-09-12': {
    contextWindow: 128000,
    maxOutput: 65536,
    name: 'o1 Mini (Sep 2024)'
  },
  
  // o3 Series
  'o3-mini': {
    contextWindow: 200000,
    maxOutput: 100000,
    name: 'o3 Mini'
  },
  'o3-mini-2025-01-31': {
    contextWindow: 200000,
    maxOutput: 100000,
    name: 'o3 Mini (Jan 2025)'
  },
}

export interface AvailableModel {
  id: string
  name: string
  contextWindow: number
  maxOutput: number
  supportsVision?: boolean
  supportsAudio?: boolean
  ownedBy: string
}

const DEFAULT_SYSTEM_PROMPT = `You are a helpful AI assistant integrated into a desktop overlay. 
You provide concise, actionable responses. Keep responses brief and focused.
When analyzing screenshots, provide specific insights and recommendations.
When translating, maintain context and nuance.
Always be respectful and professional.`

export class OpenAIService {
  private client: OpenAI | null = null
  private apiKey: string = ''
  private audioContext: AudioContext | null = null

  constructor(apiKey: string) {
    this.apiKey = apiKey
    this.initializeClient()
  }

  private initializeClient(): void {
    if (!this.apiKey) {
      throw new Error('API key is required')
    }

    this.client = new OpenAI({
      apiKey: this.apiKey,
      dangerouslyAllowBrowser: true,
    })
  }

  async streamChat(
    messages: AIMessage[],
    options: StreamOptions = {},
    onChunk: (chunk: string) => void,
    onError: (error: string) => void,
    onComplete: (usage?: TokenUsage) => void
  ): Promise<void> {
    if (!this.client) {
      onError('OpenAI client not initialized')
      return
    }

    const {
      model = 'gpt-4o',
      temperature = 0.7,
      maxTokens = 500,
      systemPrompt = DEFAULT_SYSTEM_PROMPT,
    } = options

    try {
      const formattedMessages = [
        { role: 'system' as const, content: systemPrompt },
        ...messages.map((msg) => ({
          role: msg.role as 'user' | 'assistant' | 'system',
          content: msg.content,
        })),
      ]

      const stream = await this.client.chat.completions.create({
        model,
        messages: formattedMessages,
        stream: true,
        stream_options: { include_usage: true }, // Incluir uso de tokens no stream
        temperature,
        max_tokens: maxTokens,
      })

      let tokenUsage: TokenUsage | undefined
      let chunkCount = 0

      for await (const chunk of stream) {
        chunkCount++
        const content = chunk.choices[0]?.delta?.content || ''
        if (content) {
          onChunk(content)
        }
        
        // Capturar uso de tokens quando dispon√≠vel (√∫ltimo chunk)
        // O usage vem no √∫ltimo chunk quando stream_options.include_usage = true
        if (chunk.usage) {
          tokenUsage = {
            promptTokens: chunk.usage.prompt_tokens,
            completionTokens: chunk.usage.completion_tokens,
            totalTokens: chunk.usage.total_tokens,
            cachedTokens: (chunk.usage as any).prompt_tokens_details?.cached_tokens
          }
          console.log('[OpenAI] ‚úÖ Token usage received:', tokenUsage)
        }
        
        // Debug: log √∫ltimo chunk para verificar estrutura
        if (chunk.choices[0]?.finish_reason) {
          console.log('[OpenAI] üìä Stream finished, chunk count:', chunkCount, 'finish_reason:', chunk.choices[0].finish_reason)
          console.log('[OpenAI] üìä Final chunk has usage?', !!chunk.usage, chunk.usage ? JSON.stringify(chunk.usage) : 'N/A')
        }
      }

      console.log('[OpenAI] üèÅ Stream complete, tokenUsage:', tokenUsage ? JSON.stringify(tokenUsage) : 'undefined')
      onComplete(tokenUsage)
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred'
      onError(errorMessage)
    }
  }

  async translateText(
    text: string,
    targetLanguage: string = 'Portuguese'
  ): Promise<string> {
    if (!this.client) {
      throw new Error('OpenAI client not initialized')
    }

    try {
      const response = await this.client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'user',
            content: `Translate the following text to ${targetLanguage}. Return only the translation, no explanations:\n\n${text}`,
          },
        ],
        temperature: 0.3,
        max_tokens: 500,
      })

      return response.choices[0]?.message?.content || ''
    } catch (error) {
      throw new Error(`Translation failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
    }
  }

  async analyzeImage(
    imageBase64: string,
    prompt: string = 'Analyze this image and provide insights'
  ): Promise<string> {
    if (!this.client) {
      throw new Error('OpenAI client not initialized')
    }

    try {
      const response = await this.client.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: prompt,
              },
              {
                type: 'image_url',
                image_url: {
                  url: `data:image/jpeg;base64,${imageBase64}`,
                  detail: 'auto',
                },
              },
            ],
          },
        ],
        max_tokens: 1024,
      })

      return response.choices[0]?.message?.content || ''
    } catch (error) {
      throw new Error(`Image analysis failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
    }
  }

  // Fun√ß√£o para obter ou criar AudioContext reutiliz√°vel
  private getAudioContext(): AudioContext {
    if (!this.audioContext || this.audioContext.state === 'closed') {
      this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
    }
    return this.audioContext
  }

  // Fun√ß√£o para converter √°udio para WAV usando Web Audio API
  private async convertToWav(audioBlob: Blob): Promise<Blob> {
    try {
      console.log('[Convert] Convertendo √°udio para WAV...')
      
      // Usar AudioContext reutiliz√°vel
      const audioContext = this.getAudioContext()
      
      // Converter blob para ArrayBuffer
      const arrayBuffer = await audioBlob.arrayBuffer()
      
      // Criar uma c√≥pia do ArrayBuffer para evitar problemas de reutiliza√ß√£o
      const bufferCopy = arrayBuffer.slice(0)
      
      // Decodificar √°udio
      const audioBuffer = await audioContext.decodeAudioData(bufferCopy)
      
      // Configura√ß√µes WAV
      const sampleRate = audioBuffer.sampleRate
      const numberOfChannels = audioBuffer.numberOfChannels
      const length = audioBuffer.length
      
      console.log('[Convert] Audio info:', { sampleRate, numberOfChannels, length })
      
      // Criar buffer WAV
      const wavBuffer = this.createWavBuffer(audioBuffer, sampleRate, numberOfChannels)
      
      const wavBlob = new Blob([wavBuffer], { type: 'audio/wav' })
      console.log('[Convert] WAV criado:', wavBlob.size, 'bytes')
      
      return wavBlob
    } catch (error) {
      console.error('[Convert] Erro na convers√£o:', error)
      
      // Se falhar na convers√£o, tentar estrat√©gias alternativas
      console.log('[Convert] Tentando estrat√©gia alternativa...')
      
      try {
        // Estrat√©gia 1: Usar o blob original com tipo correto
        if (audioBlob.type.includes('webm') || audioBlob.type.includes('opus')) {
          console.log('[Convert] Usando WebM/Opus original')
          return new Blob([audioBlob], { type: 'audio/webm' })
        }
        
        // Estrat√©gia 2: For√ßar como MP3 se poss√≠vel
        if (audioBlob.size > 1000) { // S√≥ se tiver conte√∫do significativo
          console.log('[Convert] Tentando como MP3')
          return new Blob([audioBlob], { type: 'audio/mp3' })
        }
        
      } catch (fallbackError) {
        console.error('[Convert] Erro na estrat√©gia alternativa:', fallbackError)
      }
      
      // √öltima tentativa: retornar o blob original
      return audioBlob
    }
  }

  // Fun√ß√£o para criar buffer WAV
  private createWavBuffer(audioBuffer: AudioBuffer, sampleRate: number, numberOfChannels: number): ArrayBuffer {
    const length = audioBuffer.length
    const arrayBuffer = new ArrayBuffer(44 + length * numberOfChannels * 2)
    const view = new DataView(arrayBuffer)
    
    // Header WAV
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    // RIFF header
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + length * numberOfChannels * 2, true)
    writeString(8, 'WAVE')
    
    // fmt chunk
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true) // chunk size
    view.setUint16(20, 1, true) // PCM format
    view.setUint16(22, numberOfChannels, true)
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, sampleRate * numberOfChannels * 2, true) // byte rate
    view.setUint16(32, numberOfChannels * 2, true) // block align
    view.setUint16(34, 16, true) // bits per sample
    
    // data chunk
    writeString(36, 'data')
    view.setUint32(40, length * numberOfChannels * 2, true)
    
    // Audio data
    let offset = 44
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numberOfChannels; channel++) {
        const sample = Math.max(-1, Math.min(1, audioBuffer.getChannelData(channel)[i]))
        view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true)
        offset += 2
      }
    }
    
    return arrayBuffer
  }

  async transcribeAudio(audioBlob: Blob, language: string = 'pt'): Promise<{ text: string; audioUrl: string }> {
    if (!this.client) {
      throw new Error('OpenAI client not initialized')
    }

    // Vari√°veis para capturar resultado antes de qualquer erro
    let transcriptionText = ''
    let audioUrl = ''
    let transcriptionSuccess = false

    try {
      console.log('[Transcribe] Original blob:', audioBlob.type, audioBlob.size, 'bytes')
      
      // Converter blob para base64 data URL para persist√™ncia
      // Isso permite que o √°udio seja salvo no banco e carregado posteriormente
      audioUrl = await this.blobToDataUrl(audioBlob)
      console.log('[Transcribe] Audio converted to data URL, length:', audioUrl.length)
      
      // Estrat√©gia m√∫ltipla: tentar v√°rios formatos at√© um funcionar
      const strategies = [
        // Estrat√©gia 1: Converter para WAV
        async () => {
          console.log('[Transcribe] Estrat√©gia 1: Convertendo para WAV...')
          const wavBlob = await this.convertToWav(audioBlob)
          return new File([wavBlob], `audio_${Date.now()}.wav`, { type: 'audio/wav' })
        },
        
        // Estrat√©gia 2: Usar WebM original
        async () => {
          console.log('[Transcribe] Estrat√©gia 2: Usando WebM original...')
          return new File([audioBlob], `audio_${Date.now()}.webm`, { type: 'audio/webm' })
        },
        
        // Estrat√©gia 3: For√ßar como MP3
        async () => {
          console.log('[Transcribe] Estrat√©gia 3: Tentando como MP3...')
          return new File([audioBlob], `audio_${Date.now()}.mp3`, { type: 'audio/mp3' })
        },
        
        // Estrat√©gia 4: Usar OGG
        async () => {
          console.log('[Transcribe] Estrat√©gia 4: Tentando como OGG...')
          return new File([audioBlob], `audio_${Date.now()}.ogg`, { type: 'audio/ogg' })
        }
      ]
      
      let lastError: any = null
      
      // Tentar cada estrat√©gia at√© uma funcionar
      for (let i = 0; i < strategies.length; i++) {
        try {
          console.log(`[Transcribe] Tentando estrat√©gia ${i + 1}/${strategies.length}...`)
          
          const audioFile = await strategies[i]()
          console.log('[Transcribe] File created:', audioFile.name, audioFile.type, audioFile.size)
          
          // Tentar transcrever com este formato
          const response = await this.client.audio.transcriptions.create({
            file: audioFile,
            model: 'whisper-1',
            language: language,
            response_format: 'text',
            temperature: 0.0,
          })
          
          // Se chegou at√© aqui, funcionou!
          transcriptionText = response
          transcriptionSuccess = true
          
          console.log(`[Transcribe] ‚úÖ Estrat√©gia ${i + 1} funcionou! Response length:`, response.length)
          console.log('[Transcribe] ‚úÖ Response text:', response.substring(0, 100) + '...')
          
          return {
            text: transcriptionText,
            audioUrl: audioUrl
          }
          
        } catch (strategyError) {
          console.warn(`[Transcribe] ‚ùå Estrat√©gia ${i + 1} falhou:`, strategyError)
          lastError = strategyError
          
          // Continuar para pr√≥xima estrat√©gia
          continue
        }
      }
      
      // Se chegou aqui, todas as estrat√©gias falharam
      throw lastError || new Error('Todas as estrat√©gias de convers√£o falharam')
      
    } catch (error) {
      console.error('[Transcribe] Error details:', error)
      
      // CR√çTICO: Se temos texto transcrito, retornar sucesso mesmo com erro
      if (transcriptionSuccess && transcriptionText.trim()) {
        console.log('[Transcribe] üîÑ Retornando sucesso apesar do erro (texto capturado)')
        return {
          text: transcriptionText,
          audioUrl: audioUrl
        }
      }
      
      // Se realmente n√£o temos texto, tratar como erro mas N√ÉO lan√ßar exce√ß√£o
      console.log('[Transcribe] üí• Todas as estrat√©gias falharam - sem texto capturado')
      
      let errorMessage = 'Unknown error'
      if (error instanceof Error) {
        errorMessage = error.message
        
        // Se for erro de formato, dar informa√ß√£o mais espec√≠fica
        if (errorMessage.includes('Invalid file format') || errorMessage.includes('format') || errorMessage.includes('could not be decoded')) {
          errorMessage = `Todas as estrat√©gias de formato falharam. √öltimo erro: ${errorMessage}`
        }
      }
      
      // IMPORTANTE: Retornar objeto vazio ao inv√©s de lan√ßar erro
      // Isso evita que a grava√ß√£o pare
      return {
        text: '', // Texto vazio indica falha
        audioUrl: audioUrl, // URL ainda funciona para debug
        error: errorMessage // Adicionar campo de erro opcional
      } as any
    }
  }

  // M√©todo para converter Blob para Data URL (base64)
  // Isso permite persistir o √°udio no banco de dados
  private blobToDataUrl(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader()
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result)
        } else {
          reject(new Error('Failed to convert blob to data URL'))
        }
      }
      reader.onerror = () => reject(reader.error)
      reader.readAsDataURL(blob)
    })
  }

  // M√©todo para limpar recursos
  cleanup(): void {
    if (this.audioContext && this.audioContext.state !== 'closed') {
      this.audioContext.close()
      this.audioContext = null
    }
  }

  setApiKey(apiKey: string): void {
    this.apiKey = apiKey
    this.initializeClient()
  }

  isInitialized(): boolean {
    return this.client !== null && this.apiKey !== ''
  }

  /**
   * Lista os modelos de chat dispon√≠veis na conta do usu√°rio
   * Combina dados da API com informa√ß√µes de contexto locais
   */
  async listAvailableModels(): Promise<AvailableModel[]> {
    if (!this.client) {
      throw new Error('OpenAI client not initialized')
    }

    try {
      const response = await this.client.models.list()
      
      // Filtrar apenas modelos de chat (gpt-*, o1-*, o3-*, chatgpt-*)
      const chatModels = response.data.filter(model => {
        const id = model.id.toLowerCase()
        return (
          (id.startsWith('gpt-') ||
           id.startsWith('o1') ||
           id.startsWith('o3') ||
           id.startsWith('chatgpt-')) &&
          // Excluir modelos de embedding, instruct, etc
          !id.includes('instruct') &&
          !id.includes('embedding') &&
          !id.includes('realtime') &&
          !id.includes('audio') &&
          !id.includes('search') &&
          !id.includes('similarity') &&
          !id.includes('edit') &&
          !id.includes('insert')
        )
      })

      // Mapear para AvailableModel com informa√ß√µes de contexto
      const availableModels: AvailableModel[] = chatModels.map(model => {
        const contextInfo = MODEL_CONTEXT_INFO[model.id]
        
        // Se n√£o temos info de contexto, usar valores padr√£o baseados no nome
        const defaultContext = this.getDefaultContextForModel(model.id)
        
        return {
          id: model.id,
          name: contextInfo?.name || this.formatModelName(model.id),
          contextWindow: contextInfo?.contextWindow || defaultContext.contextWindow,
          maxOutput: contextInfo?.maxOutput || defaultContext.maxOutput,
          supportsVision: contextInfo?.supportsVision || model.id.includes('4o') || model.id.includes('vision'),
          supportsAudio: contextInfo?.supportsAudio || false,
          ownedBy: model.owned_by
        }
      })

      // Ordenar por nome (modelos mais recentes primeiro)
      return availableModels.sort((a, b) => {
        // Priorizar modelos principais
        const priority = ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-3.5-turbo', 'o1', 'o1-mini', 'o3-mini']
        const aIndex = priority.findIndex(p => a.id.startsWith(p))
        const bIndex = priority.findIndex(p => b.id.startsWith(p))
        
        if (aIndex !== -1 && bIndex !== -1) return aIndex - bIndex
        if (aIndex !== -1) return -1
        if (bIndex !== -1) return 1
        
        return a.name.localeCompare(b.name)
      })
    } catch (error) {
      console.error('[OpenAI] Error listing models:', error)
      throw new Error(`Failed to list models: ${error instanceof Error ? error.message : 'Unknown error'}`)
    }
  }

  /**
   * Obt√©m informa√ß√µes de contexto para um modelo espec√≠fico
   */
  getModelContextInfo(modelId: string): ModelContextInfo | null {
    return MODEL_CONTEXT_INFO[modelId] || null
  }

  /**
   * Retorna valores padr√£o de contexto baseado no nome do modelo
   */
  private getDefaultContextForModel(modelId: string): { contextWindow: number; maxOutput: number } {
    const id = modelId.toLowerCase()
    
    // o1/o3 series - contexto muito grande
    if (id.startsWith('o1') || id.startsWith('o3')) {
      return { contextWindow: 128000, maxOutput: 65536 }
    }
    
    // GPT-4o series
    if (id.includes('4o')) {
      return { contextWindow: 128000, maxOutput: 16384 }
    }
    
    // GPT-4 Turbo
    if (id.includes('4-turbo')) {
      return { contextWindow: 128000, maxOutput: 4096 }
    }
    
    // GPT-4 32K
    if (id.includes('4-32k')) {
      return { contextWindow: 32768, maxOutput: 4096 }
    }
    
    // GPT-4 base
    if (id.includes('gpt-4')) {
      return { contextWindow: 8192, maxOutput: 4096 }
    }
    
    // GPT-3.5
    if (id.includes('3.5')) {
      return { contextWindow: 16385, maxOutput: 4096 }
    }
    
    // Fallback
    return { contextWindow: 8192, maxOutput: 4096 }
  }

  /**
   * Formata o nome do modelo para exibi√ß√£o
   */
  private formatModelName(modelId: string): string {
    return modelId
      .replace('gpt-', 'GPT-')
      .replace('-turbo', ' Turbo')
      .replace('-preview', ' Preview')
      .replace('-mini', ' Mini')
      .replace('chatgpt-', 'ChatGPT-')
      .replace(/(\d{4})-(\d{2})-(\d{2})/, '($1-$2-$3)')
  }

  /**
   * Estima o n√∫mero de tokens em um texto
   * Usa aproxima√ß√£o de ~4 caracteres por token para ingl√™s
   * Para portugu√™s/outros idiomas, usa ~3 caracteres por token
   */
  estimateTokens(text: string): number {
    // Detectar se √© majoritariamente ASCII (ingl√™s) ou n√£o
    const asciiChars = text.replace(/[^\x00-\x7F]/g, '').length
    const totalChars = text.length
    const asciiRatio = asciiChars / totalChars

    // Usar ratio diferente baseado no idioma
    const charsPerToken = asciiRatio > 0.8 ? 4 : 3
    
    return Math.ceil(text.length / charsPerToken)
  }

  /**
   * Estima tokens de uma lista de mensagens
   */
  estimateMessagesTokens(messages: AIMessage[]): number {
    let totalTokens = 0
    
    for (const msg of messages) {
      // Overhead por mensagem (~4 tokens para role, separadores, etc)
      totalTokens += 4
      
      // Conte√∫do da mensagem
      totalTokens += this.estimateTokens(msg.content)
      
      // Se o conte√∫do cont√©m base64 de imagem, adicionar tokens extras
      if (msg.content.includes('data:image')) {
        totalTokens += 170 // Imagens consomem ~85-170 tokens
      }
    }
    
    // Overhead do sistema (~3 tokens)
    totalTokens += 3
    
    return totalTokens
  }

  /**
   * Prompt padr√£o para condensamento inteligente de contexto
   */
  static readonly CONDENSE_SYSTEM_PROMPT = `Voc√™ √© um especialista em resumir conversas mantendo o contexto essencial para continuidade fluida.

OBJETIVO: Criar um resumo que permita ao assistente continuar a conversa naturalmente, como se tivesse acesso ao hist√≥rico completo.

REGRAS DE CONDENSAMENTO:
1. **Contexto do Usu√°rio**: Capture quem √© o usu√°rio, suas prefer√™ncias, estilo de comunica√ß√£o
2. **T√≥picos Discutidos**: Liste os principais assuntos abordados
3. **Decis√µes Tomadas**: Registre qualquer decis√£o ou conclus√£o importante
4. **Tarefas Pendentes**: Anote tarefas mencionadas mas n√£o conclu√≠das
5. **Informa√ß√µes T√©cnicas**: Preserve detalhes t√©cnicos relevantes (c√≥digo, configura√ß√µes, etc)
6. **Tom da Conversa**: Indique se √© formal, casual, t√©cnico, etc

FORMATO DO RESUMO:
üìã **Contexto Geral**: [Breve descri√ß√£o do contexto]
üë§ **Sobre o Usu√°rio**: [Prefer√™ncias e estilo identificados]
üìù **T√≥picos Principais**:
- [T√≥pico 1]
- [T√≥pico 2]
‚úÖ **Decis√µes/Conclus√µes**:
- [Decis√£o 1]
‚è≥ **Pend√™ncias**:
- [Tarefa pendente]
üí° **Notas Importantes**: [Detalhes t√©cnicos ou contextuais relevantes]

IMPORTANTE: O resumo deve ser conciso mas completo o suficiente para que a conversa continue sem perda de contexto.`

  /**
   * Condensa mensagens antigas em um resumo para economizar contexto
   */
  async condenseMessages(
    messages: AIMessage[],
    maxTokens: number = 2000
  ): Promise<{ summary: string; condensedMessages: AIMessage[]; usage?: TokenUsage }> {
    if (!this.client) {
      throw new Error('OpenAI client not initialized')
    }

    // Separar mensagens recentes (manter intactas) das antigas (condensar)
    const recentCount = Math.min(4, Math.floor(messages.length / 2))
    const recentMessages = messages.slice(-recentCount)
    const oldMessages = messages.slice(0, -recentCount)

    if (oldMessages.length === 0) {
      return {
        summary: '',
        condensedMessages: messages
      }
    }

    // Criar texto das mensagens antigas para resumir
    const oldConversation = oldMessages
      .map(m => `${m.role === 'user' ? 'Usu√°rio' : 'Assistente'}: ${m.content}`)
      .join('\n\n')

    try {
      const response = await this.client.chat.completions.create({
        model: 'gpt-4o-mini', // Usar modelo mais barato mas capaz para resumo
        messages: [
          {
            role: 'system',
            content: OpenAIService.CONDENSE_SYSTEM_PROMPT
          },
          {
            role: 'user',
            content: `Resuma a seguinte conversa (${oldMessages.length} mensagens) para permitir continuidade fluida:\n\n${oldConversation}`
          }
        ],
        temperature: 0.3,
        max_tokens: maxTokens
      })

      const summary = response.choices[0]?.message?.content || ''
      
      // Capturar uso de tokens
      const usage: TokenUsage | undefined = response.usage ? {
        promptTokens: response.usage.prompt_tokens,
        completionTokens: response.usage.completion_tokens,
        totalTokens: response.usage.total_tokens
      } : undefined

      // Criar mensagem de sistema com o resumo
      const summaryMessage: AIMessage = {
        role: 'system',
        content: `[üìö Contexto Condensado - ${oldMessages.length} mensagens resumidas]\n\n${summary}\n\n[Fim do resumo - Continue a conversa naturalmente]`
      }

      console.log(`[OpenAI] Condensamento: ${oldMessages.length} mensagens ‚Üí resumo de ${summary.length} chars`)
      if (usage) {
        console.log(`[OpenAI] Tokens usados no condensamento: ${usage.totalTokens}`)
      }

      return {
        summary,
        condensedMessages: [summaryMessage, ...recentMessages],
        usage
      }
    } catch (error) {
      console.error('[OpenAI] Error condensing messages:', error)
      // Em caso de erro, retornar mensagens originais
      return {
        summary: '',
        condensedMessages: messages
      }
    }
  }
}
